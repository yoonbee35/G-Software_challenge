{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugCwAjPNZxu3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, Input, BatchNormalization, Multiply, Activation\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "import os\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2843,
     "status": "ok",
     "timestamp": 1577809449870,
     "user": {
      "displayName": "정윤비",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDPo7rDpuCFll8oq3tnoAes6qCXpMz_g3Q7sW4Q=s64",
      "userId": "01138123058524543991"
     },
     "user_tz": -540
    },
    "id": "2GqCtLZ9oVVt",
    "outputId": "0e99cbcd-5eb0-4efc-a52f-6c01d47085c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24800 images belonging to 31 classes.\n",
      "Found 6200 images belonging to 31 classes.\n"
     ]
    }
   ],
   "source": [
    "#train files 전처리\n",
    "train_datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                 samplewise_center=False,\n",
    "                 featurewise_std_normalization=False,\n",
    "                 samplewise_std_normalization=False,\n",
    "                 zca_whitening=False,\n",
    "                 rotation_range=10,\n",
    "                 width_shift_range=0.05,\n",
    "                 height_shift_range=0.05,\n",
    "                 shear_range=0.1,\n",
    "                 zoom_range=0.2,\n",
    "                 channel_shift_range=0.,\n",
    "                 fill_mode='nearest',\n",
    "                 cval=0.,\n",
    "                 horizontal_flip=True,\n",
    "                 vertical_flip=False,\n",
    "                 rescale=1/255,\n",
    "                 validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        \"food-101\\images\",\n",
    "        target_size=(224,224),\n",
    "        batch_size=64,\n",
    "        subset='training') # set as training data\n",
    "\n",
    "#validation set 생성\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    \"food-101\\images\",\n",
    "    target_size=(224,224),\n",
    "    batch_size=64,\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9uxpOluoiEt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0105 21:14:34.604406  7816 deprecation_wrapper.py:119] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0105 21:14:34.620414  7816 deprecation_wrapper.py:119] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0105 21:14:34.622556  7816 deprecation_wrapper.py:119] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0105 21:14:34.651416  7816 deprecation_wrapper.py:119] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0105 21:14:34.654421  7816 deprecation_wrapper.py:119] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0105 21:14:34.661418  7816 deprecation.py:506] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0105 21:14:34.902475  7816 deprecation_wrapper.py:119] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0105 21:14:34.924476  7816 deprecation_wrapper.py:119] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Modeling\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), strides = 2, padding = 'Same', activation ='relu', input_shape = (224,224,3), kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), strides = 2, padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(filters = 256, kernel_size = (2,2),padding = 'Same', activation ='relu',kernel_initializer='he_normal'))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(512, activation = \"relu\",kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(31, activation = \"softmax\",kernel_initializer='he_normal',kernel_regularizer=l2()))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1577809650814,
     "user": {
      "displayName": "정윤비",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDPo7rDpuCFll8oq3tnoAes6qCXpMz_g3Q7sW4Q=s64",
      "userId": "01138123058524543991"
     },
     "user_tz": -540
    },
    "id": "vHdB8UNYp63-",
    "outputId": "dbaa76b5-6e9d-40cb-a474-d65c75ac93c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 32)      2432      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       32896     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 256)         131328    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 256)         262400    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 31)                15903     \n",
      "=================================================================\n",
      "Total params: 723,263\n",
      "Trainable params: 723,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 727870,
     "status": "ok",
     "timestamp": 1577814786829,
     "user": {
      "displayName": "정윤비",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDPo7rDpuCFll8oq3tnoAes6qCXpMz_g3Q7sW4Q=s64",
      "userId": "01138123058524543991"
     },
     "user_tz": -540
    },
    "id": "f1GRokXQqYTL",
    "outputId": "0c99ff31-0c6e-47e5-957e-f6f0a5f20e3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 21:14:35.046502  7816 deprecation.py:323] From C:\\Users\\Owner2\\Anaconda3\\envs\\keras2\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "200/200 [==============================] - 216s 1s/step - loss: 3.6076 - acc: 0.0309 - val_loss: 3.4600 - val_acc: 0.0312\n",
      "Epoch 2/200\n",
      "200/200 [==============================] - 212s 1s/step - loss: 3.4124 - acc: 0.0449 - val_loss: 3.3397 - val_acc: 0.0547\n",
      "Epoch 3/200\n",
      "200/200 [==============================] - 198s 988ms/step - loss: 3.3426 - acc: 0.0560 - val_loss: 3.2963 - val_acc: 0.0664\n",
      "Epoch 4/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 3.2866 - acc: 0.0759 - val_loss: 3.2817 - val_acc: 0.0755\n",
      "Epoch 5/200\n",
      "200/200 [==============================] - 197s 983ms/step - loss: 3.2464 - acc: 0.0880 - val_loss: 3.2741 - val_acc: 0.0846\n",
      "Epoch 6/200\n",
      "200/200 [==============================] - 198s 989ms/step - loss: 3.2028 - acc: 0.1013 - val_loss: 3.2099 - val_acc: 0.1068\n",
      "Epoch 7/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 3.1735 - acc: 0.1070 - val_loss: 3.2528 - val_acc: 0.1315\n",
      "Epoch 8/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 3.1419 - acc: 0.1186 - val_loss: 3.1400 - val_acc: 0.1510\n",
      "Epoch 9/200\n",
      "200/200 [==============================] - 200s 998ms/step - loss: 3.0923 - acc: 0.1353 - val_loss: 3.1517 - val_acc: 0.1303\n",
      "Epoch 10/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 3.0510 - acc: 0.1465 - val_loss: 3.1079 - val_acc: 0.1510\n",
      "Epoch 11/200\n",
      "200/200 [==============================] - 198s 992ms/step - loss: 2.9955 - acc: 0.1648 - val_loss: 2.9250 - val_acc: 0.1758\n",
      "Epoch 12/200\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 2.9448 - acc: 0.1777 - val_loss: 3.0211 - val_acc: 0.1628\n",
      "Epoch 13/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 2.8740 - acc: 0.1919 - val_loss: 2.8422 - val_acc: 0.2018\n",
      "Epoch 14/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 2.7988 - acc: 0.2107 - val_loss: 2.7329 - val_acc: 0.2448\n",
      "Epoch 15/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 2.7327 - acc: 0.2265 - val_loss: 2.7095 - val_acc: 0.2240\n",
      "Epoch 16/200\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 2.6924 - acc: 0.2313 - val_loss: 2.6934 - val_acc: 0.2422\n",
      "Epoch 17/200\n",
      "200/200 [==============================] - 197s 987ms/step - loss: 2.6182 - acc: 0.2524 - val_loss: 2.6278 - val_acc: 0.2592\n",
      "Epoch 18/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 2.5811 - acc: 0.2623 - val_loss: 2.5298 - val_acc: 0.2943\n",
      "Epoch 19/200\n",
      "200/200 [==============================] - 198s 991ms/step - loss: 2.5350 - acc: 0.2699 - val_loss: 2.5124 - val_acc: 0.2799\n",
      "Epoch 20/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 2.4897 - acc: 0.2862 - val_loss: 2.6241 - val_acc: 0.2812\n",
      "Epoch 21/200\n",
      "200/200 [==============================] - 198s 988ms/step - loss: 2.4580 - acc: 0.2981 - val_loss: 2.4076 - val_acc: 0.3008\n",
      "Epoch 22/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 2.4258 - acc: 0.3075 - val_loss: 2.3231 - val_acc: 0.3268\n",
      "Epoch 23/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 2.3918 - acc: 0.3232 - val_loss: 2.3751 - val_acc: 0.3255\n",
      "Epoch 24/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 2.3638 - acc: 0.3207 - val_loss: 2.4059 - val_acc: 0.3125\n",
      "Epoch 25/200\n",
      "200/200 [==============================] - 198s 992ms/step - loss: 2.3171 - acc: 0.3436 - val_loss: 2.2156 - val_acc: 0.3711\n",
      "Epoch 26/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 2.3017 - acc: 0.3456 - val_loss: 2.3015 - val_acc: 0.3542\n",
      "Epoch 27/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 2.2614 - acc: 0.3620 - val_loss: 2.1852 - val_acc: 0.4076\n",
      "Epoch 28/200\n",
      "200/200 [==============================] - 200s 999ms/step - loss: 2.2391 - acc: 0.3678 - val_loss: 2.2053 - val_acc: 0.3802\n",
      "Epoch 29/200\n",
      "200/200 [==============================] - 199s 996ms/step - loss: 2.2038 - acc: 0.3733 - val_loss: 2.2701 - val_acc: 0.3672\n",
      "Epoch 30/200\n",
      "200/200 [==============================] - 198s 989ms/step - loss: 2.1381 - acc: 0.3900 - val_loss: 2.1267 - val_acc: 0.3958\n",
      "Epoch 31/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 2.1551 - acc: 0.3888 - val_loss: 2.1310 - val_acc: 0.3932\n",
      "Epoch 32/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 2.0928 - acc: 0.4081 - val_loss: 2.1977 - val_acc: 0.3763\n",
      "Epoch 33/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 2.0836 - acc: 0.4066 - val_loss: 2.2035 - val_acc: 0.3816\n",
      "Epoch 34/200\n",
      "200/200 [==============================] - 205s 1s/step - loss: 2.0606 - acc: 0.4141 - val_loss: 2.1370 - val_acc: 0.3997\n",
      "Epoch 35/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 2.0438 - acc: 0.4182 - val_loss: 2.0200 - val_acc: 0.4284\n",
      "Epoch 36/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 2.0268 - acc: 0.4181 - val_loss: 2.1645 - val_acc: 0.3945\n",
      "Epoch 37/200\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 1.9983 - acc: 0.4362 - val_loss: 2.0892 - val_acc: 0.4180\n",
      "Epoch 38/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.9701 - acc: 0.4422 - val_loss: 1.9995 - val_acc: 0.4453\n",
      "Epoch 39/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.9524 - acc: 0.4470 - val_loss: 2.0142 - val_acc: 0.4258\n",
      "Epoch 40/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.9148 - acc: 0.4544 - val_loss: 1.9765 - val_acc: 0.4635\n",
      "Epoch 41/200\n",
      "200/200 [==============================] - 196s 978ms/step - loss: 1.9165 - acc: 0.4530 - val_loss: 2.0402 - val_acc: 0.4000\n",
      "Epoch 42/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.8907 - acc: 0.4650 - val_loss: 2.1743 - val_acc: 0.3893\n",
      "Epoch 43/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 1.8477 - acc: 0.4723 - val_loss: 1.9362 - val_acc: 0.4531\n",
      "Epoch 44/200\n",
      "200/200 [==============================] - 200s 1000ms/step - loss: 1.8376 - acc: 0.4809 - val_loss: 1.8755 - val_acc: 0.4466\n",
      "Epoch 45/200\n",
      "200/200 [==============================] - 206s 1s/step - loss: 1.8343 - acc: 0.4809 - val_loss: 1.8677 - val_acc: 0.4792\n",
      "Epoch 46/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.7987 - acc: 0.4909 - val_loss: 1.8505 - val_acc: 0.4844\n",
      "Epoch 47/200\n",
      "200/200 [==============================] - 199s 994ms/step - loss: 1.8165 - acc: 0.4835 - val_loss: 1.8331 - val_acc: 0.4883\n",
      "Epoch 48/200\n",
      "200/200 [==============================] - 206s 1s/step - loss: 1.7604 - acc: 0.5039 - val_loss: 1.8248 - val_acc: 0.4974\n",
      "Epoch 49/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.7786 - acc: 0.4954 - val_loss: 2.0307 - val_acc: 0.4539\n",
      "Epoch 50/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.7648 - acc: 0.4961 - val_loss: 1.6990 - val_acc: 0.5130\n",
      "Epoch 51/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.7241 - acc: 0.5118 - val_loss: 1.7831 - val_acc: 0.5065\n",
      "Epoch 52/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.7046 - acc: 0.5167 - val_loss: 1.9125 - val_acc: 0.4961\n",
      "Epoch 53/200\n",
      "200/200 [==============================] - 196s 981ms/step - loss: 1.7074 - acc: 0.5164 - val_loss: 1.6828 - val_acc: 0.5312\n",
      "Epoch 54/200\n",
      "200/200 [==============================] - 200s 1000ms/step - loss: 1.6976 - acc: 0.5224 - val_loss: 1.8555 - val_acc: 0.4792\n",
      "Epoch 55/200\n",
      "200/200 [==============================] - 195s 975ms/step - loss: 1.6740 - acc: 0.5217 - val_loss: 1.8076 - val_acc: 0.4974\n",
      "Epoch 56/200\n",
      "200/200 [==============================] - 198s 992ms/step - loss: 1.6754 - acc: 0.5268 - val_loss: 1.7966 - val_acc: 0.4935\n",
      "Epoch 57/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.6307 - acc: 0.5387 - val_loss: 1.8601 - val_acc: 0.5026\n",
      "Epoch 58/200\n",
      "200/200 [==============================] - 199s 996ms/step - loss: 1.6475 - acc: 0.5309 - val_loss: 1.8310 - val_acc: 0.5078\n",
      "Epoch 59/200\n",
      "200/200 [==============================] - 206s 1s/step - loss: 1.6145 - acc: 0.5424 - val_loss: 1.8250 - val_acc: 0.5013\n",
      "Epoch 60/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.6299 - acc: 0.5397 - val_loss: 1.8027 - val_acc: 0.5078\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 199s 994ms/step - loss: 1.6076 - acc: 0.5403 - val_loss: 1.7394 - val_acc: 0.5312\n",
      "Epoch 62/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.6052 - acc: 0.5408 - val_loss: 1.7889 - val_acc: 0.5312\n",
      "Epoch 63/200\n",
      "200/200 [==============================] - 197s 983ms/step - loss: 1.5687 - acc: 0.5518 - val_loss: 1.7541 - val_acc: 0.5234\n",
      "Epoch 64/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.5722 - acc: 0.5567 - val_loss: 1.7289 - val_acc: 0.5326\n",
      "Epoch 65/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.5494 - acc: 0.5618 - val_loss: 1.6993 - val_acc: 0.5447\n",
      "Epoch 66/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 1.5814 - acc: 0.5530 - val_loss: 1.9531 - val_acc: 0.4948\n",
      "Epoch 67/200\n",
      "200/200 [==============================] - 198s 991ms/step - loss: 1.5441 - acc: 0.5618 - val_loss: 1.8376 - val_acc: 0.4909\n",
      "Epoch 68/200\n",
      "200/200 [==============================] - 200s 1000ms/step - loss: 1.5579 - acc: 0.5551 - val_loss: 1.6210 - val_acc: 0.5547\n",
      "Epoch 69/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.5188 - acc: 0.5670 - val_loss: 1.7668 - val_acc: 0.5326\n",
      "Epoch 70/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 1.5326 - acc: 0.5713 - val_loss: 1.7529 - val_acc: 0.5169\n",
      "Epoch 71/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.5005 - acc: 0.5741 - val_loss: 1.6082 - val_acc: 0.5495\n",
      "Epoch 72/200\n",
      "200/200 [==============================] - 200s 1000ms/step - loss: 1.4960 - acc: 0.5680 - val_loss: 1.6843 - val_acc: 0.5469\n",
      "Epoch 73/200\n",
      "200/200 [==============================] - 200s 998ms/step - loss: 1.4934 - acc: 0.5777 - val_loss: 1.7127 - val_acc: 0.5592\n",
      "Epoch 74/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.5077 - acc: 0.5720 - val_loss: 1.7374 - val_acc: 0.5143\n",
      "Epoch 75/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.4684 - acc: 0.5812 - val_loss: 1.6866 - val_acc: 0.5378\n",
      "Epoch 76/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 1.4662 - acc: 0.5880 - val_loss: 1.6460 - val_acc: 0.5911\n",
      "Epoch 77/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 1.4563 - acc: 0.5840 - val_loss: 1.6186 - val_acc: 0.5495\n",
      "Epoch 78/200\n",
      "200/200 [==============================] - 200s 998ms/step - loss: 1.4514 - acc: 0.5880 - val_loss: 1.7436 - val_acc: 0.5273\n",
      "Epoch 79/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.4448 - acc: 0.5898 - val_loss: 1.7525 - val_acc: 0.5234\n",
      "Epoch 80/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 1.4328 - acc: 0.5913 - val_loss: 1.6503 - val_acc: 0.5430\n",
      "Epoch 81/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.4329 - acc: 0.5889 - val_loss: 1.7117 - val_acc: 0.5513\n",
      "Epoch 82/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 1.4500 - acc: 0.5866 - val_loss: 1.5696 - val_acc: 0.5625\n",
      "Epoch 83/200\n",
      "200/200 [==============================] - 198s 989ms/step - loss: 1.4104 - acc: 0.5991 - val_loss: 1.6644 - val_acc: 0.5690\n",
      "Epoch 84/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 1.4119 - acc: 0.5968 - val_loss: 1.6383 - val_acc: 0.5378\n",
      "Epoch 85/200\n",
      "200/200 [==============================] - 199s 993ms/step - loss: 1.4117 - acc: 0.5952 - val_loss: 1.6790 - val_acc: 0.5417\n",
      "Epoch 86/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.4062 - acc: 0.5985 - val_loss: 1.6354 - val_acc: 0.5521\n",
      "Epoch 87/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.4150 - acc: 0.5943 - val_loss: 1.6513 - val_acc: 0.5495\n",
      "Epoch 88/200\n",
      "200/200 [==============================] - 198s 992ms/step - loss: 1.3913 - acc: 0.6001 - val_loss: 1.5715 - val_acc: 0.5638\n",
      "Epoch 89/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 1.3838 - acc: 0.6030 - val_loss: 1.6441 - val_acc: 0.5447\n",
      "Epoch 90/200\n",
      "200/200 [==============================] - 196s 979ms/step - loss: 1.3710 - acc: 0.6112 - val_loss: 1.5787 - val_acc: 0.5703\n",
      "Epoch 91/200\n",
      "200/200 [==============================] - 200s 998ms/step - loss: 1.3613 - acc: 0.6096 - val_loss: 1.6638 - val_acc: 0.5599\n",
      "Epoch 92/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.3587 - acc: 0.6179 - val_loss: 1.7314 - val_acc: 0.5286\n",
      "Epoch 93/200\n",
      "200/200 [==============================] - 199s 997ms/step - loss: 1.4039 - acc: 0.6008 - val_loss: 1.7519 - val_acc: 0.5378\n",
      "Epoch 94/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 1.3191 - acc: 0.6241 - val_loss: 1.5373 - val_acc: 0.5703\n",
      "Epoch 95/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.3742 - acc: 0.6088 - val_loss: 1.6463 - val_acc: 0.5625\n",
      "Epoch 96/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.3240 - acc: 0.6184 - val_loss: 1.6225 - val_acc: 0.5560\n",
      "Epoch 97/200\n",
      "200/200 [==============================] - 196s 980ms/step - loss: 1.3603 - acc: 0.6109 - val_loss: 1.5453 - val_acc: 0.5697\n",
      "Epoch 98/200\n",
      "200/200 [==============================] - 197s 987ms/step - loss: 1.3067 - acc: 0.6285 - val_loss: 1.6515 - val_acc: 0.5573\n",
      "Epoch 99/200\n",
      "200/200 [==============================] - 200s 999ms/step - loss: 1.3190 - acc: 0.6239 - val_loss: 1.7004 - val_acc: 0.5547\n",
      "Epoch 100/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.2953 - acc: 0.6234 - val_loss: 1.6281 - val_acc: 0.5807\n",
      "Epoch 101/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.3439 - acc: 0.6161 - val_loss: 1.5749 - val_acc: 0.5833\n",
      "Epoch 102/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.3004 - acc: 0.6259 - val_loss: 1.6425 - val_acc: 0.5729\n",
      "Epoch 103/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 1.3038 - acc: 0.6327 - val_loss: 1.5763 - val_acc: 0.5872\n",
      "Epoch 104/200\n",
      "200/200 [==============================] - 197s 987ms/step - loss: 1.2772 - acc: 0.6356 - val_loss: 1.7941 - val_acc: 0.5299\n",
      "Epoch 105/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.2923 - acc: 0.6283 - val_loss: 1.7272 - val_acc: 0.5508\n",
      "Epoch 106/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.2825 - acc: 0.6320 - val_loss: 1.5839 - val_acc: 0.5789\n",
      "Epoch 107/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.2939 - acc: 0.6278 - val_loss: 1.5993 - val_acc: 0.5677\n",
      "Epoch 108/200\n",
      "200/200 [==============================] - 196s 979ms/step - loss: 1.2594 - acc: 0.6337 - val_loss: 1.5580 - val_acc: 0.5625\n",
      "Epoch 109/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 1.2819 - acc: 0.6305 - val_loss: 1.5993 - val_acc: 0.5703\n",
      "Epoch 110/200\n",
      "200/200 [==============================] - 199s 996ms/step - loss: 1.2898 - acc: 0.6298 - val_loss: 1.7183 - val_acc: 0.5521\n",
      "Epoch 111/200\n",
      "200/200 [==============================] - 200s 998ms/step - loss: 1.2646 - acc: 0.6383 - val_loss: 1.6230 - val_acc: 0.5781\n",
      "Epoch 112/200\n",
      "200/200 [==============================] - 196s 981ms/step - loss: 1.2612 - acc: 0.6401 - val_loss: 1.6924 - val_acc: 0.5755\n",
      "Epoch 113/200\n",
      "200/200 [==============================] - 197s 987ms/step - loss: 1.2444 - acc: 0.6394 - val_loss: 1.6149 - val_acc: 0.5573\n",
      "Epoch 114/200\n",
      "200/200 [==============================] - 199s 996ms/step - loss: 1.2636 - acc: 0.6354 - val_loss: 1.5658 - val_acc: 0.5645\n",
      "Epoch 115/200\n",
      "200/200 [==============================] - 200s 999ms/step - loss: 1.2617 - acc: 0.6354 - val_loss: 1.7167 - val_acc: 0.5560\n",
      "Epoch 116/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.2463 - acc: 0.6472 - val_loss: 1.4339 - val_acc: 0.6003\n",
      "Epoch 117/200\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 1.2364 - acc: 0.6456 - val_loss: 1.7345 - val_acc: 0.5482\n",
      "Epoch 118/200\n",
      "200/200 [==============================] - 199s 997ms/step - loss: 1.2424 - acc: 0.6461 - val_loss: 1.7460 - val_acc: 0.5443\n",
      "Epoch 119/200\n",
      "200/200 [==============================] - 206s 1s/step - loss: 1.2240 - acc: 0.6508 - val_loss: 1.6694 - val_acc: 0.5807\n",
      "Epoch 120/200\n",
      "200/200 [==============================] - 207s 1s/step - loss: 1.2423 - acc: 0.6445 - val_loss: 1.7343 - val_acc: 0.5260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.2301 - acc: 0.6514 - val_loss: 1.6993 - val_acc: 0.5495\n",
      "Epoch 122/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.2576 - acc: 0.6441 - val_loss: 1.5617 - val_acc: 0.5882\n",
      "Epoch 123/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.1967 - acc: 0.6565 - val_loss: 1.7460 - val_acc: 0.5339\n",
      "Epoch 124/200\n",
      "200/200 [==============================] - 199s 996ms/step - loss: 1.2351 - acc: 0.6452 - val_loss: 1.7178 - val_acc: 0.5495\n",
      "Epoch 125/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.1939 - acc: 0.6542 - val_loss: 1.5900 - val_acc: 0.5820\n",
      "Epoch 126/200\n",
      "200/200 [==============================] - 200s 998ms/step - loss: 1.2008 - acc: 0.6570 - val_loss: 1.5755 - val_acc: 0.5951\n",
      "Epoch 127/200\n",
      "200/200 [==============================] - 199s 993ms/step - loss: 1.1771 - acc: 0.6589 - val_loss: 1.5766 - val_acc: 0.5729\n",
      "Epoch 128/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.2203 - acc: 0.6536 - val_loss: 1.5422 - val_acc: 0.5820\n",
      "Epoch 129/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1962 - acc: 0.6560 - val_loss: 1.6182 - val_acc: 0.5612\n",
      "Epoch 130/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 1.2045 - acc: 0.6479 - val_loss: 1.7389 - val_acc: 0.5329\n",
      "Epoch 131/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.1767 - acc: 0.6587 - val_loss: 1.7016 - val_acc: 0.5547\n",
      "Epoch 132/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1994 - acc: 0.6630 - val_loss: 1.6649 - val_acc: 0.5807\n",
      "Epoch 133/200\n",
      "200/200 [==============================] - 199s 994ms/step - loss: 1.1849 - acc: 0.6577 - val_loss: 1.6541 - val_acc: 0.5664\n",
      "Epoch 134/200\n",
      "200/200 [==============================] - 197s 984ms/step - loss: 1.1877 - acc: 0.6604 - val_loss: 1.7347 - val_acc: 0.5547\n",
      "Epoch 135/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1794 - acc: 0.6623 - val_loss: 1.5723 - val_acc: 0.5924\n",
      "Epoch 136/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 1.1708 - acc: 0.6600 - val_loss: 1.7266 - val_acc: 0.5651\n",
      "Epoch 137/200\n",
      "200/200 [==============================] - 200s 1000ms/step - loss: 1.1785 - acc: 0.6630 - val_loss: 1.5734 - val_acc: 0.5924\n",
      "Epoch 138/200\n",
      "200/200 [==============================] - 206s 1s/step - loss: 1.1451 - acc: 0.6712 - val_loss: 1.7003 - val_acc: 0.5697\n",
      "Epoch 139/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1581 - acc: 0.6677 - val_loss: 1.6342 - val_acc: 0.5755\n",
      "Epoch 140/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1904 - acc: 0.6611 - val_loss: 1.6311 - val_acc: 0.5742\n",
      "Epoch 141/200\n",
      "200/200 [==============================] - 199s 997ms/step - loss: 1.1275 - acc: 0.6790 - val_loss: 1.5520 - val_acc: 0.6003\n",
      "Epoch 142/200\n",
      "200/200 [==============================] - 197s 984ms/step - loss: 1.1528 - acc: 0.6730 - val_loss: 1.5742 - val_acc: 0.5586\n",
      "Epoch 143/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 1.1495 - acc: 0.6681 - val_loss: 1.7335 - val_acc: 0.5703\n",
      "Epoch 144/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1350 - acc: 0.6727 - val_loss: 1.6226 - val_acc: 0.6042\n",
      "Epoch 145/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1585 - acc: 0.6660 - val_loss: 1.7320 - val_acc: 0.5599\n",
      "Epoch 146/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 1.1279 - acc: 0.6699 - val_loss: 1.5138 - val_acc: 0.5947\n",
      "Epoch 147/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.1405 - acc: 0.6724 - val_loss: 1.6939 - val_acc: 0.5703\n",
      "Epoch 148/200\n",
      "200/200 [==============================] - 205s 1s/step - loss: 1.1277 - acc: 0.6749 - val_loss: 1.6437 - val_acc: 0.5794\n",
      "Epoch 149/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1471 - acc: 0.6745 - val_loss: 1.4422 - val_acc: 0.6133\n",
      "Epoch 150/200\n",
      "200/200 [==============================] - 200s 999ms/step - loss: 1.0949 - acc: 0.6848 - val_loss: 1.6279 - val_acc: 0.5964\n",
      "Epoch 151/200\n",
      "200/200 [==============================] - 197s 987ms/step - loss: 1.1350 - acc: 0.6731 - val_loss: 1.6431 - val_acc: 0.5495\n",
      "Epoch 152/200\n",
      "200/200 [==============================] - 198s 989ms/step - loss: 1.0796 - acc: 0.6890 - val_loss: 1.7116 - val_acc: 0.5677\n",
      "Epoch 153/200\n",
      "200/200 [==============================] - 199s 996ms/step - loss: 1.1248 - acc: 0.6784 - val_loss: 1.7443 - val_acc: 0.5625\n",
      "Epoch 154/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 1.1244 - acc: 0.6798 - val_loss: 1.5646 - val_acc: 0.6066\n",
      "Epoch 155/200\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 1.1140 - acc: 0.6800 - val_loss: 1.6070 - val_acc: 0.5807\n",
      "Epoch 156/200\n",
      "200/200 [==============================] - 197s 984ms/step - loss: 1.0872 - acc: 0.6875 - val_loss: 1.5820 - val_acc: 0.5664\n",
      "Epoch 157/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1220 - acc: 0.6777 - val_loss: 1.6432 - val_acc: 0.5651\n",
      "Epoch 158/200\n",
      "200/200 [==============================] - 199s 997ms/step - loss: 1.0987 - acc: 0.6838 - val_loss: 1.4952 - val_acc: 0.6211\n",
      "Epoch 159/200\n",
      "200/200 [==============================] - 198s 991ms/step - loss: 1.1243 - acc: 0.6780 - val_loss: 1.6328 - val_acc: 0.5964\n",
      "Epoch 160/200\n",
      "200/200 [==============================] - 199s 994ms/step - loss: 1.0952 - acc: 0.6829 - val_loss: 1.7010 - val_acc: 0.5964\n",
      "Epoch 161/200\n",
      "200/200 [==============================] - 198s 991ms/step - loss: 1.0873 - acc: 0.6884 - val_loss: 1.5801 - val_acc: 0.5794\n",
      "Epoch 162/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 1.0762 - acc: 0.6898 - val_loss: 1.8483 - val_acc: 0.5474\n",
      "Epoch 163/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.1083 - acc: 0.6865 - val_loss: 1.7365 - val_acc: 0.5638\n",
      "Epoch 164/200\n",
      "200/200 [==============================] - 200s 999ms/step - loss: 1.0834 - acc: 0.6878 - val_loss: 1.6380 - val_acc: 0.5833\n",
      "Epoch 165/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.0929 - acc: 0.6874 - val_loss: 1.7112 - val_acc: 0.5885\n",
      "Epoch 166/200\n",
      "200/200 [==============================] - 198s 991ms/step - loss: 1.0681 - acc: 0.6943 - val_loss: 1.6676 - val_acc: 0.5742\n",
      "Epoch 167/200\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 1.0891 - acc: 0.6860 - val_loss: 1.5958 - val_acc: 0.6016\n",
      "Epoch 168/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 1.0945 - acc: 0.6866 - val_loss: 1.5928 - val_acc: 0.5924\n",
      "Epoch 169/200\n",
      "200/200 [==============================] - 196s 982ms/step - loss: 1.0810 - acc: 0.6867 - val_loss: 1.6736 - val_acc: 0.5807\n",
      "Epoch 170/200\n",
      "200/200 [==============================] - 200s 1000ms/step - loss: 1.0747 - acc: 0.6912 - val_loss: 1.7306 - val_acc: 0.6000\n",
      "Epoch 171/200\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 1.0796 - acc: 0.6916 - val_loss: 1.7071 - val_acc: 0.5977\n",
      "Epoch 172/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 1.0486 - acc: 0.7033 - val_loss: 1.6736 - val_acc: 0.5846\n",
      "Epoch 173/200\n",
      "200/200 [==============================] - 199s 994ms/step - loss: 1.0735 - acc: 0.6930 - val_loss: 1.5072 - val_acc: 0.6172\n",
      "Epoch 174/200\n",
      "200/200 [==============================] - 200s 1s/step - loss: 1.0859 - acc: 0.6910 - val_loss: 1.4179 - val_acc: 0.6276\n",
      "Epoch 175/200\n",
      "200/200 [==============================] - 200s 999ms/step - loss: 1.0607 - acc: 0.6946 - val_loss: 1.5738 - val_acc: 0.5977\n",
      "Epoch 176/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 1.0731 - acc: 0.6922 - val_loss: 1.5637 - val_acc: 0.5924\n",
      "Epoch 177/200\n",
      "200/200 [==============================] - 199s 994ms/step - loss: 1.0611 - acc: 0.6993 - val_loss: 1.8767 - val_acc: 0.5378\n",
      "Epoch 178/200\n",
      "200/200 [==============================] - 205s 1s/step - loss: 1.0603 - acc: 0.7008 - val_loss: 1.7724 - val_acc: 0.5539\n",
      "Epoch 179/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.0432 - acc: 0.6991 - val_loss: 1.6409 - val_acc: 0.5807\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 201s 1s/step - loss: 1.0682 - acc: 0.6939 - val_loss: 1.6953 - val_acc: 0.5729\n",
      "Epoch 181/200\n",
      "200/200 [==============================] - 199s 993ms/step - loss: 1.0478 - acc: 0.6960 - val_loss: 1.7463 - val_acc: 0.5560\n",
      "Epoch 182/200\n",
      "200/200 [==============================] - 200s 999ms/step - loss: 1.0572 - acc: 0.6934 - val_loss: 1.5390 - val_acc: 0.6224\n",
      "Epoch 183/200\n",
      "200/200 [==============================] - 198s 990ms/step - loss: 1.0414 - acc: 0.6988 - val_loss: 1.6649 - val_acc: 0.5938\n",
      "Epoch 184/200\n",
      "200/200 [==============================] - 198s 992ms/step - loss: 1.0504 - acc: 0.6959 - val_loss: 1.6631 - val_acc: 0.5703\n",
      "Epoch 185/200\n",
      "200/200 [==============================] - 197s 983ms/step - loss: 1.0362 - acc: 0.6992 - val_loss: 1.6003 - val_acc: 0.5990\n",
      "Epoch 186/200\n",
      "200/200 [==============================] - 196s 978ms/step - loss: 1.0297 - acc: 0.7059 - val_loss: 1.6339 - val_acc: 0.5961\n",
      "Epoch 187/200\n",
      "200/200 [==============================] - 202s 1s/step - loss: 1.0291 - acc: 0.7017 - val_loss: 1.8228 - val_acc: 0.5560\n",
      "Epoch 188/200\n",
      "200/200 [==============================] - 200s 998ms/step - loss: 1.0318 - acc: 0.7019 - val_loss: 1.6967 - val_acc: 0.5664\n",
      "Epoch 189/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.0265 - acc: 0.7044 - val_loss: 1.6431 - val_acc: 0.5885\n",
      "Epoch 190/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.0519 - acc: 0.6933 - val_loss: 1.6546 - val_acc: 0.6107\n",
      "Epoch 191/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.0249 - acc: 0.7022 - val_loss: 1.8648 - val_acc: 0.5612\n",
      "Epoch 192/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.0432 - acc: 0.7013 - val_loss: 1.6337 - val_acc: 0.5716\n",
      "Epoch 193/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.0095 - acc: 0.7055 - val_loss: 1.7114 - val_acc: 0.5794\n",
      "Epoch 194/200\n",
      "200/200 [==============================] - 200s 998ms/step - loss: 1.0438 - acc: 0.7024 - val_loss: 1.6294 - val_acc: 0.6066\n",
      "Epoch 195/200\n",
      "200/200 [==============================] - 205s 1s/step - loss: 1.0381 - acc: 0.7008 - val_loss: 1.7008 - val_acc: 0.5820\n",
      "Epoch 196/200\n",
      "200/200 [==============================] - 204s 1s/step - loss: 1.0435 - acc: 0.7002 - val_loss: 1.8151 - val_acc: 0.5638\n",
      "Epoch 197/200\n",
      "200/200 [==============================] - 199s 995ms/step - loss: 1.0127 - acc: 0.7077 - val_loss: 1.6854 - val_acc: 0.6042\n",
      "Epoch 198/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.0349 - acc: 0.7010 - val_loss: 1.6384 - val_acc: 0.6003\n",
      "Epoch 199/200\n",
      "200/200 [==============================] - 201s 1s/step - loss: 1.0252 - acc: 0.7075 - val_loss: 1.7502 - val_acc: 0.5742\n",
      "Epoch 200/200\n",
      "200/200 [==============================] - 203s 1s/step - loss: 1.0363 - acc: 0.7027 - val_loss: 1.8102 - val_acc: 0.5586\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,steps_per_epoch=250,\n",
    "                              validation_data=validation_generator,validation_steps=750/64, \n",
    "                              epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22929,
     "status": "ok",
     "timestamp": 1577814820592,
     "user": {
      "displayName": "정윤비",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDPo7rDpuCFll8oq3tnoAes6qCXpMz_g3Q7sW4Q=s64",
      "userId": "01138123058524543991"
     },
     "user_tz": -540
    },
    "id": "jcgIdshzDaK2",
    "outputId": "429e7193-93e8-4d03-9e82-3e1056933041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluate --\n",
      "acc: 58.13%\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    "\n",
    "scores = model.evaluate_generator(\n",
    "            validation_generator, \n",
    "            steps = 5)\n",
    "\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGbin8u0qecx"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('food_classfication_git_ver.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMzrk4h0F2al"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "food classfication_colab_깊은CNN_낮은epoch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
